

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon/apple-touch-icon.png">
  <link rel="icon" href="/img/favicon/favicon-32x32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ZhangHao">
  <meta name="keywords" content="多模态学习,MultiModal Learning,MMML,MultiModal Machine Learning">
  
    <meta name="description" content="吴恩达日前在DeepLearning.AI平台分享了2022年AI趋势预测，第一就是多模态AI将起飞。本文介绍多模态机器学习的定义、发展历史、主要任务和挑战，以及目前SOTA的CLIP模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="多模态学习(MultiModal Learning)">
<meta property="og:url" content="https://imzhanghao.com/2022/10/27/multimodal-learning/index.html">
<meta property="og:site_name" content="张浩在路上">
<meta property="og:description" content="吴恩达日前在DeepLearning.AI平台分享了2022年AI趋势预测，第一就是多模态AI将起飞。本文介绍多模态机器学习的定义、发展历史、主要任务和挑战，以及目前SOTA的CLIP模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211050831484.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211050837453.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051318189.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211050816652.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051318784.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051125254.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051008371.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051037668.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051034371.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051034148.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051339734.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051431550.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211052136742.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051458819.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051506598.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211052203950.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211090619157.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051434189.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051433591.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211071349090.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211071428784.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051435164.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051436546.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211051437074.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211081504107.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211081512646.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211081515556.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211081516232.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211081543158.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100602961.gif">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100600578.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100613302.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100622320.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100641357.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100631474.png">
<meta property="og:image" content="https://oss.imzhanghao.com/img/202211100631176.png">
<meta property="article:published_time" content="2022-10-26T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-10T09:53:58.738Z">
<meta property="article:author" content="ZhangHao">
<meta property="article:tag" content="多模态学习">
<meta property="article:tag" content="MultiModal Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://oss.imzhanghao.com/img/202211050831484.png">
  
  
  <title>多模态学习(MultiModal Learning) - 张浩在路上</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"imzhanghao.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"0c64a5657309290fd8f5efe33fcbcdb4","google":"UA-166608124-1","gtag":"G-PKN25ZF7R2","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="张浩在路上" type="application/atom+xml">
</head>


<body>
  <header style="height: 45vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>张浩在路上</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/technology/">
                <i class="iconfont icon-bug"></i>
                技术
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/monetization/">
                <i class="iconfont icon-briefcase"></i>
                变现
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/thinking/">
                <i class="iconfont icon-books"></i>
                思考
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="多模态学习(MultiModal Learning)">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-10-27 00:00" pubdate>
        2022年10月27日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      10k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      87 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">多模态学习(MultiModal Learning)</h1>
            
            <div class="markdown-body">
              <p>最早开始关注到多模态机器学习是看到Jeff Dean在2019年年底NeurIPS大会上的一个采访报道，讲到了2020年机器学习趋势：多任务和多模态学习将成为突破口。<br />
<img src="https://oss.imzhanghao.com/img/202211050831484.png" srcset="/img/loading.gif" lazyload alt="Jeff Dean 谈2020年机器学习趋势：多任务和多模式学习将成为突破口" /><br />
站在2022年，也正如他预言的一样，多模态学习在行业内越来越火爆。<br />
<img src="https://oss.imzhanghao.com/img/202211050837453.png" srcset="/img/loading.gif" lazyload alt="多模态机器学习在Google Trends上的表现" /></p>
<h2 id="一-定义"><a class="markdownIt-Anchor" href="#一-定义"></a> 一、定义</h2>
<p>多模态机器学习，英文全称 MultiModal Machine Learning (MMML)</p>
<p>模态（modal）是事情经历和发生的方式，我们生活在一个由多种模态（Multimodal）信息构成的世界，包括视觉信息、听觉信息、文本信息、嗅觉信息等等，当研究的问题或者数据集包含多种这样的模态信息时我们称之为多模态问题，研究多模态问题是推动人工智能更好的了解和认知我们周围世界的关键。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051318189.png" srcset="/img/loading.gif" lazyload alt="What is Mulimodel" /></p>
<h3 id="11-模态"><a class="markdownIt-Anchor" href="#11-模态"></a> 1.1 模态</h3>
<p><strong>模态是指一些表达或感知事物的方式，每一种信息的来源或者形式，都可以称为一种模态</strong>。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介，有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。以上的每一种都可以称为一种模态。</p>
<p><strong>相较于图像、语音、文本等多媒体(Multi-media)数据划分形式，“模态”是一个更为细粒度的概念，同一媒介下可存在不同的模态。</strong> 比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。</p>
<h3 id="12-多模态"><a class="markdownIt-Anchor" href="#12-多模态"></a> 1.2 多模态</h3>
<p><strong>多模态即是从多个模态表达或感知事物。</strong> 多模态可归类为同质性的模态，例如从两台相机中分别拍摄的图片，异质性的模态，例如图片与文本语言的关系。</p>
<p><strong>多模态可能有以下三种形式：</strong></p>
<ul>
<li><strong>描述同一对象的多媒体数据</strong>。如互联网环境下描述某一特定对象的视频、图片、语音、文本等信息。下图即为典型的多模态信息形式。<br />
<img src="https://oss.imzhanghao.com/img/202211050816652.png" srcset="/img/loading.gif" lazyload alt="“下雪”场景的多模态数据(图像、音频与文本)" /></li>
<li><strong>来自不同传感器的同一类媒体数据</strong>。如医学影像学中不同的检查设备所产生的图像数据， 包括B超(B-Scan ultrasonography)、计算机断层扫描(CT)、核磁共振等；物联网背景下不同传感器所检测到的同一对象数据等。</li>
<li><strong>具有不同的数据结构特点、表示形式的表意符号与信息</strong>。如描述同一对象的结构化、非结构化的数据单元；描述同一数学概念的公式、逻辑 符号、函数图及解释性文本；描述同一语义的词向量、词袋、知识图谱以及其它语义符号单元等。</li>
</ul>
<p>通常主要研究模态包括&quot;<strong>3V</strong>&quot;：即<strong>Verbal(文本)、Vocal(语音)、Visual(视觉)</strong>。<br />
人跟人交流时的多模态：<br />
<img src="https://oss.imzhanghao.com/img/202211051318784.png" srcset="/img/loading.gif" lazyload alt="multimodal communicative behaviors" /></p>
<h3 id="13-多模态学习"><a class="markdownIt-Anchor" href="#13-多模态学习"></a> 1.3 多模态学习</h3>
<p><strong>多模态机器学习</strong>是从多种模态的数据中学习并且提升自身的算法，它不是某一个具体的算法，它是一类算法的总称。</p>
<p>从<strong>语义感知</strong>的角度切入，多模态数据涉及<strong>不同的感知通道</strong>如视觉、听觉、触觉、嗅觉所接收到的信息;在<strong>数据层面</strong>理解，多模态数据则可被看作<strong>多种数据类型</strong>的组合，如图片、数值、文本、符号、音频、时间序列，或者集合、树、图等不同数据结构所组成的复合数据形式，乃至来自不同数据库、不同知识库的各种信息资源的组合。<strong>对多源异构数据的挖掘分析可被理解为多模态学习</strong>。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051125254.png" srcset="/img/loading.gif" lazyload alt="多模态学习举例" /></p>
<h2 id="二-发展历史"><a class="markdownIt-Anchor" href="#二-发展历史"></a> 二、发展历史</h2>
<p><img src="https://oss.imzhanghao.com/img/202211051008371.png" srcset="/img/loading.gif" lazyload alt="多模态发展的四个时期" /></p>
<h3 id="21-行为时代"><a class="markdownIt-Anchor" href="#21-行为时代"></a> 2.1 行为时代</h3>
<blockquote>
<p>The “behavioral” era (1970s until late 1980s)，这一阶段主要从心理学的角度对多模态这一现象进行剖析。</p>
</blockquote>
<ul>
<li>Chicago 的McNeill 认为手势是说话人的思考行为，是言语表达的重要组成部分，而不仅仅是补足。</li>
<li>1976年的McGurk效应：当语音与唇形不符合时，大脑会脑补出中和的声音MCGURK, H., MACDONALD, J. Hearing lips and seeing voices. Nature 264, 746–748 (1976). <a target="_blank" rel="noopener" href="https://www.futilitycloset.com/2019/05/30/the-mcgurk-effect/">The McGurk Effect Video</a></li>
</ul>
<h3 id="22-计算时代"><a class="markdownIt-Anchor" href="#22-计算时代"></a> 2.2 计算时代</h3>
<blockquote>
<p>The “computational” era (late 1980s until 2000)，这一阶段主要利用一些浅层的模型对多模态问题进行研究，其中代表性的应用包括视觉语音联合识别，多模态情感计算等等。</p>
</blockquote>
<ul>
<li>
<p>视频音频语音识别(AVSR)，在声音的低信噪比下，引入视觉信号能够极大提升识别准确率<br />
<img src="https://oss.imzhanghao.com/img/202211051037668.png" srcset="/img/loading.gif" lazyload alt="AVSR" /></p>
</li>
<li>
<p>多模态/多感知接口：情感计算：与情感或其他情感现象有关、源于情感或有意影响情感的计算[Rosalind Picard]</p>
</li>
<li>
<p>多媒体计算：CMU曾有过信息媒体数字视频库项目[1994-2010]，</p>
</li>
</ul>
<h3 id="23-交互时代"><a class="markdownIt-Anchor" href="#23-交互时代"></a> 2.3 交互时代</h3>
<blockquote>
<p>The “interaction” era (2000 - 2010)，这一阶段主要主要从交互的角度入手，研究多模态识别问题，其中主要的代表作品包括苹果的语音助手Siri等。</p>
</blockquote>
<p>拟人类多模态交互过程</p>
<ul>
<li>
<p>IDIAP实验室的AMI项目[2001-2006]，记录会议录音、同步音频视频、转录与注释；</p>
</li>
<li>
<p>Alex Waibel的CHIL项目，将计算机置于人类交互圈中，多传感器多模态信号处理，面对面交互<br />
<img src="https://oss.imzhanghao.com/img/202211051034371.png" srcset="/img/loading.gif" lazyload alt="IMI Projet &amp; CHIL Project" /></p>
</li>
<li>
<p>2003-2008 SRI的学习和组织认知助手，个性化助手，Siri就是这个项目的衍生产品</p>
</li>
<li>
<p>2008-2011 IDIAP的社交信号处理网络，数据库http://sspnet.eu。<br />
<img src="https://oss.imzhanghao.com/img/202211051034148.png" srcset="/img/loading.gif" lazyload alt="CALO Project &amp; SSP Project" /></p>
</li>
</ul>
<h3 id="24-深度学习时代"><a class="markdownIt-Anchor" href="#24-深度学习时代"></a> 2.4 深度学习时代</h3>
<blockquote>
<p>The “deep learning” era (2010s until …)，促使多模态研究发展的关键促成因素有4个，1）新的大规模多模态数据集，2）GPU快速计算，3）强大的视觉特征抽取能力，4）强大的语言特征抽取能力。</p>
</blockquote>
<p>表示学习三篇参考文献</p>
<ul>
<li>Multimodal Deep Learning [ICML 2011]</li>
<li>Multimodal Learning with Deep Boltzmann Machines [NIPS 2012]</li>
<li>Visual attention: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention [ICML 2015]</li>
</ul>
<h2 id="三-典型任务"><a class="markdownIt-Anchor" href="#三-典型任务"></a> 三、典型任务</h2>
<h3 id="31-跨模态预训练"><a class="markdownIt-Anchor" href="#31-跨模态预训练"></a> 3.1 跨模态预训练</h3>
<ul>
<li>图像/视频与语言预训练。</li>
<li>跨任务预训练</li>
</ul>
<h3 id="32-language-audio"><a class="markdownIt-Anchor" href="#32-language-audio"></a> 3.2 Language-Audio</h3>
<ul>
<li>Text-to-Speech Synthesis: 给定文本，生成一段对应的声音。</li>
<li>Audio Captioning：给定一段语音，生成一句话总结并描述主要内容。(不是语音识别)</li>
</ul>
<h3 id="33-vision-audio"><a class="markdownIt-Anchor" href="#33-vision-audio"></a> 3.3 Vision-Audio</h3>
<ul>
<li>Audio-Visual Speech Recognition(视听语音识别)：给定某人的视频及语音进行语音识别。</li>
<li>Video Sound Separation(视频声源分离)：给定视频和声音信号(包含多个声源)，进行声源定位与分离。</li>
<li>Image Generation from Audio: 给定声音，生成与其相关的图像。</li>
<li>Speech-conditioned Face generation：给定一段话，生成说话人的视频。</li>
<li>Audio-Driven 3D Facial Animation：给定一段话与3D人脸模版，生成说话的人脸3D动画。</li>
</ul>
<h3 id="34-vision-language"><a class="markdownIt-Anchor" href="#34-vision-language"></a> 3.4 Vision-Language</h3>
<ul>
<li>Image/Video-Text Retrieval (图(视频)文检索): 图像/视频&lt;–&gt;文本的相互检索。</li>
<li>Image/Video Captioning(图像/视频描述)：给定一个图像/视频，生成文本描述其主要内容。</li>
<li>Visual Question Answering(视觉问答)：给定一个图像/视频与一个问题，预测答案。</li>
<li>Image/Video Generation from Text：给定文本，生成相应的图像或视频。</li>
<li>Multimodal Machine Translation：给定一种语言的文本与该文本对应的图像，翻译为另外一种语言。</li>
<li>Vision-and-Language Navigation(视觉-语言导航)： 给定自然语言进行指导，使得智能体根据视觉传感器导航到特定的目标。</li>
<li>Multimodal Dialog(多模态对话)： 给定图像，历史对话，以及与图像相关的问题，预测该问题的回答。</li>
</ul>
<h3 id="35-定位相关的任务"><a class="markdownIt-Anchor" href="#35-定位相关的任务"></a> 3.5 定位相关的任务</h3>
<ul>
<li>Visual Grounding：给定一个图像与一段文本，定位到文本所描述的物体。</li>
<li>Temporal Language Localization: 给定一个视频即一段文本，定位到文本所描述的动作(预测起止时间)。</li>
<li>Video Summarization from text query：给定一段话(query)与一个视频，根据这段话的内容进行视频摘要，预测视频关键帧(或关键片段)组合为一个短的摘要视频。</li>
<li>Video Segmentation from Natural Language Query: 给定一段话(query)与一个视频，分割得到query所指示的物体。</li>
<li>Video-Language Inference: 给定视频(包括视频的一些字幕信息)，还有一段文本假设(hypothesis)，判断二者是否存在语义蕴含(二分类)，即判断视频内容是否包含这段文本的语义。</li>
<li>Object Tracking from Natural Language Query: 给定一段视频和一些文本，追踪视频中文本所描述的对象。</li>
<li>Language-guided Image/Video Editing: 一句话自动修图。给定一段指令(文本)，自动进行图像/视频的编辑。</li>
</ul>
<h3 id="36-更多模态"><a class="markdownIt-Anchor" href="#36-更多模态"></a> 3.6 更多模态</h3>
<ul>
<li>Affect Computing (情感计算)：使用语音、视觉(人脸表情)、文本信息、心电、脑电等模态进行情感识别。</li>
<li>Medical Image：不同医疗图像模态如CT、MRI、PETRGB-D模态：RGB图与深度图</li>
</ul>
<h2 id="四-技术挑战"><a class="markdownIt-Anchor" href="#四-技术挑战"></a> 四、技术挑战</h2>
<p><img src="https://oss.imzhanghao.com/img/202211051339734.png" srcset="/img/loading.gif" lazyload alt="多模态学习的技术挑战" /></p>
<h3 id="41-表征representation"><a class="markdownIt-Anchor" href="#41-表征representation"></a> 4.1 表征Representation</h3>
<p>第一个基本挑战是学习如何以<strong>利用多种模态的互补性和冗余性的方式表示和总结多模态数据</strong>。多模态数据的异质性使得构建这样的表示具有挑战性。例如，语言通常是象征性的，而音频和视觉形式将被表示为信号。</p>
<p>单模态的表征负责将信息表示为计算机可以处理的数值向量或者进一步抽象为更高层的特征向量，而多模态表征是指通过利用多模态之间的互补性，剔除模态间的冗余性，从而学习到更好的特征表示。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051431550.png" srcset="/img/loading.gif" lazyload alt="Representation" /></p>
<h4 id="411-联合表征"><a class="markdownIt-Anchor" href="#411-联合表征"></a> 4.1.1 联合表征</h4>
<p>联合表征（Joint Representation）将多个模态的信息一起映射到一个统一的多模态向量空间，Joint结构注重捕捉多模态的<strong>互补性</strong>，融合多个输入模态<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_1,x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>获得多模态表征<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>m</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_m=f(x_1,...,x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，进而使<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">x_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>完成某种预测任务。<br />
<img src="https://oss.imzhanghao.com/img/202211052136742.png" srcset="/img/loading.gif" lazyload alt="Joint Representation" /></p>
<p><strong>Multimodal learning with deep boltzmann machines (NIPS 2012)</strong> 提出将 deep boltzmann machines（DBM） 结构扩充到多模态领域，通过 Multimodal DBM，可以学习到多模态的联合概率分布。<br />
<img src="https://oss.imzhanghao.com/img/202211051458819.png" srcset="/img/loading.gif" lazyload alt="Multimodal DBM 模型" /></p>
<p>在获得图像与文本间的联合概率分布后，我们在应用阶段，输入图片，利用条件概率 P(文本|图片)，生成文本特征，可以得到图片相应的文本描述；而输入文本，利用条件概率 P(图片|文本)，可以生成图片特征，通过检索出最靠近该特征向量的两个图片实例，可以得到符合文本描述的图片。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051506598.png" srcset="/img/loading.gif" lazyload alt="Multimodal DBM 应用" /></p>
<h4 id="412-协同表征"><a class="markdownIt-Anchor" href="#412-协同表征"></a> 4.1.2 协同表征</h4>
<p>协同表征（Coordinated Representation）将多模态中的每个模态分别映射到各自的表示空间，但映射后的向量之间满足一定的相关性约束（例如线性相关）。Coordinated结构并不寻求融合而是建模多种模态数据间的<strong>相关性</strong>，它将多个(通常是两个)模态映射到协作空间，表示为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">～</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x_1) ～ g(x_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord cjk_fallback">～</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中～表示一种协作关系。网络的优化目标是这种协作关系(通常是相似性，即最小化cosine距离等度量)。<br />
<img src="https://oss.imzhanghao.com/img/202211052203950.png" srcset="/img/loading.gif" lazyload alt="Coordinated Representation" /></p>
<p><strong>Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (NIPS 2014)</strong> 利用协同学习到的特征向量之间满足加减算数运算这一特性，可以搜索出与给定图片满足“<strong>指定的转换语义</strong>”的图片。例如：狗的图片特征向量 - 狗的文本特征向量 + 猫的文本特征向量 = 猫的图片特征向量 -&gt; 在特征向量空间，根据最近邻距离，检索得到猫的图片。<br />
<img src="https://oss.imzhanghao.com/img/202211090619157.png" srcset="/img/loading.gif" lazyload alt="多模态向量空间运算" /></p>
<h3 id="42-翻译translation"><a class="markdownIt-Anchor" href="#42-翻译translation"></a> 4.2 翻译Translation</h3>
<p>第二个挑战涉及<strong>如何将数据从一种模式转化（映射）到另一种模式</strong>。不仅数据是异构的，而且模态之间的关系通常是开放式的或主观的。例如，存在多种描述图像的正确方法，并且可能不存在一种完美的翻译。</p>
<h4 id="421-常见应用"><a class="markdownIt-Anchor" href="#421-常见应用"></a> 4.2.1 常见应用</h4>
<ul>
<li><strong>机器翻译（Machine Translation）</strong>：将输入的语言A（即时）翻译为另一种语言B。类似的还有唇读（Lip Reading）和语音翻译 （Speech Translation），分别将唇部视觉和语音信息转换为文本信息。</li>
<li><strong>图片描述（Image captioning) 或者视频描述（Video captioning)</strong>： 对给定的图片/视频形成一段文字描述，以表达图片/视频的内容。</li>
<li><strong>语音合成（Speech Synthesis）</strong>：根据输入的文本信息，自动合成一段语音信号。</li>
</ul>
<p><img src="https://oss.imzhanghao.com/img/202211051434189.png" srcset="/img/loading.gif" lazyload alt="Translation" /></p>
<h4 id="422-基于实例的方法"><a class="markdownIt-Anchor" href="#422-基于实例的方法"></a> 4.2.2 基于实例的方法</h4>
<p>基于实例的方法从词典中<strong>检索</strong>最佳翻译，词典一般指训练集中的数据对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">{</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>N</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">}</mo></mrow><annotation encoding="application/x-tex">\left\{\left(x_1, y_1\right), \ldots,\left(x_N, y_N\right)\right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span>。给定测试样本<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span></span>，模版法直接检索在词典中找到最匹配的翻译结果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，并将其作为最终输出。<br />
<strong>检索可分为单模态检索或跨模态检索</strong>：</p>
<ul>
<li><strong>单模态检索</strong>首先找到与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span></span>最相似的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，然后获得<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>对应的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>；</li>
<li><strong>多模态检索</strong>直接在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo fence="true">}</mo></mrow><annotation encoding="application/x-tex">\left\{y_1, \ldots, y_N\right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span>集合中检索到与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span></span>最相似的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，性能通常优于单模态检索。</li>
</ul>
<p>为进一步增强检索结果的准确性，可选择top-K的检索结果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">{</mo><msub><mi>y</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo separator="true">,</mo><msub><mi>y</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo fence="true">}</mo></mrow><annotation encoding="application/x-tex">\left\{y_{i_1}, y_{i_2}, \ldots, y_{i_k}\right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00586em;vertical-align:-0.25586em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.55em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25586em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span>，再融合K个结果作为最终输出。</p>
<h4 id="423-模型驱动的方法"><a class="markdownIt-Anchor" href="#423-模型驱动的方法"></a> 4.2.3 模型驱动的方法</h4>
<p>基于模型的首先在字典上训练一个翻译模型，然后使用该模型进行翻译。</p>
<ul>
<li>
<p><strong>基于语法的模型（Grammar-based models）</strong><br />
即人为设定多个针对目标模态的语法<strong>模版</strong>，将模型的预测结果插入模版中作为翻译结果。以图像描述为例，模版定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext> </mtext><munder accentunder="true"><mtext>who</mtext><mo stretchy="true">‾</mo></munder><mtext> did </mtext><munder accentunder="true"><mtext>what</mtext><mo stretchy="true">‾</mo></munder><mtext> to </mtext><munder accentunder="true"><mtext>whom</mtext><mo stretchy="true">‾</mo></munder><mtext> in a </mtext><munder accentunder="true"><mtext>place</mtext><mo stretchy="true">‾</mo></munder><mtext> </mtext></mrow><annotation encoding="application/x-tex">\text { \underline{who} did \underline{what} to \underline{whom} in a \underline{place} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.08888em;vertical-align:-0.39444em;"></span><span class="mord text"><span class="mord"> </span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.84em;"><span class="pstrut" style="height:3em;"></span><span class="underline-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">w</span><span class="mord">h</span><span class="mord">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20000000000000007em;"><span></span></span></span></span></span><span class="mord"> did </span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.84em;"><span class="pstrut" style="height:3em;"></span><span class="underline-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">w</span><span class="mord">h</span><span class="mord">a</span><span class="mord">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20000000000000007em;"><span></span></span></span></span></span><span class="mord"> to </span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.84em;"><span class="pstrut" style="height:3em;"></span><span class="underline-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">w</span><span class="mord">h</span><span class="mord">o</span><span class="mord">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20000000000000007em;"><span></span></span></span></span></span><span class="mord"> in a </span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944399999999998em;"><span style="top:-2.64556em;"><span class="pstrut" style="height:3em;"></span><span class="underline-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">p</span><span class="mord">l</span><span class="mord">a</span><span class="mord">c</span><span class="mord">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.39444em;"><span></span></span></span></span></span><span class="mord"> </span></span></span></span></span>，其中有四个待替换的插槽。通过不同类型的目标/属性/场景检测器可以获得who, what, whom, place等具体单词，进而完成翻译。</p>
</li>
<li>
<p><strong>编码-解码器模型（Encoder-decoder models）</strong><br />
首先将源模态的数据编码为<strong>隐特征</strong><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span>，后续被解码器用于生成目标模态。以图像描述为例，编码器(一般为CNN+spatial pooling)将图像编码为一个或多个特征向量，进而输入到RNN中以自回归的方式生成单词序列。</p>
</li>
<li>
<p><strong>连续型生成模型（Continuous generation models）</strong><br />
它针对源模态与目标模态都为流数据且在时间上严格<strong>对齐</strong>的任务。以文本合成语音为例，它与图像描述不同，语音数据与文本数据在时间上严格对齐。WaveNet采用了CNN并行预测解决该类问题，当然，编码-解码器理论上也可完成该任务，但需处理数据对齐问题。</p>
</li>
</ul>
<h4 id="424-翻译的评估困境"><a class="markdownIt-Anchor" href="#424-翻译的评估困境"></a> 4.2.4 翻译的评估困境</h4>
<p>多模态翻译方法面临的一个主要挑战是它们很难评估。语音识别等任务只有一个正确的翻译，而语音合成和媒体描述等任务则没有。有时，就像在语言翻译中，多重答案是正确的，决定哪个翻译更好往往是主观的。</p>
<ul>
<li><strong>人工评价</strong>是最理想的评估，但是耗时耗钱，且需要多样化打分人群的背景以避免偏见。</li>
<li><strong>自动化指标</strong>是视觉描述领域常用的替代方法，包括BLEU，Meteor，CIDEr，ROUGE等，但它们被证实与人的评价相关性较弱。</li>
<li><strong>基于检索的评估</strong>和<strong>弱化任务</strong>(例如：将图像描述中一对多映射简化为VQA中一对一的映射)也是解决评估困境的手段。</li>
</ul>
<h3 id="43-对齐alignment"><a class="markdownIt-Anchor" href="#43-对齐alignment"></a> 4.3 对齐Alignment</h3>
<p>第三个挑战是从<strong>两种或多种不同的模态中识别（子）元素之间的直接关系</strong>。例如，我们可能希望将食谱中的步骤与显示正在制作的菜肴的视频对齐。为了应对这一挑战，我们需要测量不同模式之间的相似性并处理可能的长期依赖和歧义。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051433591.png" srcset="/img/loading.gif" lazyload alt="Alignment" /></p>
<h4 id="431-显式对齐"><a class="markdownIt-Anchor" href="#431-显式对齐"></a> 4.3.1 显式对齐</h4>
<p>如果模型的<strong>主要目标是对齐来自两个或多个模态的子元素</strong>，那么我们将其分类为执行显式对齐。显式对齐的一个重要工作是<strong>相似性度量</strong>。大多数方法都依赖于度量不同模态的子组件之间的相似性作为基本构建块。<br />
<img src="https://oss.imzhanghao.com/img/202211071349090.png" srcset="/img/loading.gif" lazyload alt="显式对齐" /></p>
<p>包括无监督和弱监督的方法：</p>
<ul>
<li><strong>无监督对齐</strong>：给定两个模态的数据作为输入，希望模型实现子元素的对齐，但是训练数据没有“对齐结果”的标注，模型需要同时学习相似度度量和对齐方式。</li>
<li><strong>有监督对齐</strong>：有监督方法存在标注，可训练模型学习相似度度量。</li>
</ul>
<h4 id="432-隐式对齐"><a class="markdownIt-Anchor" href="#432-隐式对齐"></a> 4.3.2 隐式对齐</h4>
<p>隐式对齐<strong>用作另一个任务的中间(通常是潜在的)步骤。</strong> 这允许在许多任务中有更好的表现，包括语音识别、机器翻译、媒体描述和视觉问题回答。这些模型不显式地对齐数据，也不依赖于监督对齐示例，而是学习如何在模型训练期间潜在地对齐数据。</p>
<p><img src="https://oss.imzhanghao.com/img/202211071428784.png" srcset="/img/loading.gif" lazyload alt="隐式对齐" /></p>
<h3 id="44-融合fusion"><a class="markdownIt-Anchor" href="#44-融合fusion"></a> 4.4 融合Fusion</h3>
<p>第四个挑战是<strong>结合来自两个或多个模态的信息</strong>来执行预测。例如，对于视听语音识别，将嘴唇运动的视觉描述与语音信号融合以预测口语。来自不同模态的信息可能具有不同的预测能力和噪声拓扑，并且可能在至少一种模态中丢失数据。</p>
<p><img src="https://oss.imzhanghao.com/img/202211051435164.png" srcset="/img/loading.gif" lazyload alt="Fusion" /></p>
<p><img src="https://oss.imzhanghao.com/img/202211051436546.png" srcset="/img/loading.gif" lazyload alt="Fusion" /></p>
<h4 id="441-模型无关的方法"><a class="markdownIt-Anchor" href="#441-模型无关的方法"></a> 4.4.1 模型无关的方法</h4>
<ul>
<li><strong>早期融合（Early Fusion）</strong>：指在模型的浅层(或输入层)将多个模态的特征拼接起来，然后再级联深度网络结构，最后接上分类器或其他模型。Early Fusion 是学者对多模态融合的早期尝试，通过将各模态的底层特征进行融合学习相关性，由于只需要训练一个共同的模型，复杂度可控。但是，由于多个模态的数据来源不一致，会给<strong>拼接造成很大的难度</strong>，并且直接对原始数据进行拼接会引起较大的特征维度，对<strong>数据预处理也非常敏感</strong>。</li>
<li><strong>晚期融合（Late Fusion）</strong>：独立训练多个模型，在预测层(最后一层)进行融合，可以理解为集成方法 Ensemble Methods 的一种。Late Fusion 方式的各模态单独处理，特征独立互不影响，即使某个模态信息丢失也可以正常训练，具有很强的灵活性。但是，该方式没有充分利用模态间底层特征的相关性，并且由于涉及多个模态的分别训练，也会带来<strong>较大的计算复杂度</strong>。</li>
<li><strong>混合融合（Hybird Fusion）</strong>：同时结合前融合和后融合，以及在模型中间层进行特征交互。Hybird Fusion是一种<strong>逐级融合方式</strong>，在不同层级上依次对不同模态进行融合，综合了上述两种方式的优点，既利用了模态间信息的相关性，也具有一定的灵活性，目前大部分多模态融合都是采用这种方法。</li>
</ul>
<h4 id="442-基于模型的方法"><a class="markdownIt-Anchor" href="#442-基于模型的方法"></a> 4.4.2 基于模型的方法</h4>
<ul>
<li><strong>Deep Neural Networks</strong>：神经网络进行端到端的训练，使用LSTM、卷积层、注意力层、门机制、双线性融合等设计序列数据或图像数据的复杂交互。</li>
<li><strong>Multiple Kernel learning</strong>：多核学习（将不同的核用于不同的数据模态/视图）</li>
<li><strong>Graphical models</strong>：利用隐马尔可夫模型或贝叶斯网络建模数据的联合概率分布(生成式)或条件概率(判别式)</li>
</ul>
<h3 id="45-协同学习co-learning"><a class="markdownIt-Anchor" href="#45-协同学习co-learning"></a> 4.5 协同学习Co-learning</h3>
<p>第五个挑战是在模态的表示和它们的预测模型之间转移知识。协同学习探索了<strong>从一种模态中学习的知识如何帮助在不同模态上训练的计算模型</strong>。当其中一种模式的资源有限（例如，带注释的数据）时，这一挑战尤其重要。辅助模态（helper modality）通常只参与模型的训练过程，并不参与模型的测试使用过程</p>
<p><img src="https://oss.imzhanghao.com/img/202211051437074.png" srcset="/img/loading.gif" lazyload alt="Co-learning" /></p>
<h4 id="451-并行"><a class="markdownIt-Anchor" href="#451-并行"></a> 4.5.1 并行</h4>
<p>需要训练数据集，其中来自一种模态的观察结果与来自其他模态的观察结果直接相关，例如在一个视听语音数据集中，视频和语音样本来自同一个说话者。</p>
<h4 id="452-非并行"><a class="markdownIt-Anchor" href="#452-非并行"></a> 4.5.2 非并行</h4>
<p>不需要来自不同模式的观察结果之间的直接联系，通常通过使用类别重叠来实现共同学习，例如，在零样本学习中，使用来自Wikipedia的纯文本数据集扩展传统的视觉对象识别数据集以改进视觉对象识别的泛化能力。</p>
<h4 id="453-混合"><a class="markdownIt-Anchor" href="#453-混合"></a> 4.5.3 混合</h4>
<p>通过共享模式或数据集桥接</p>
<h2 id="五-sota模型-clip"><a class="markdownIt-Anchor" href="#五-sota模型-clip"></a> 五、SOTA模型 - CLIP</h2>
<p>CLIP全称Contrastive Language-Image Pre-training，是OpenAI最新的一篇NLP和CV结合的多模态的工作，在多模态领域迈出了重要的一步。<strong>CLIP在无需利用ImageNet的数据和标签进行训练的情况下，就可以达到ResNet50在ImageNet数据集上有监督训练的结果。</strong></p>
<p><img src="https://oss.imzhanghao.com/img/202211081504107.png" srcset="/img/loading.gif" lazyload alt="CLIP Zero shot" /></p>
<p>CLIP主要的贡献就是利用无监督的文本信息，作为监督信号来学习视觉特征。</p>
<h3 id="51-原理"><a class="markdownIt-Anchor" href="#51-原理"></a> 5.1 原理</h3>
<p><strong>CLIP不预先定义图像和文本标签类别</strong>，直接利用从互联网爬取的 400 million 个image-text pair 进行图文匹配任务的训练，并将其成功迁移应用于30个现存的计算机视觉分类。</p>
<p><img src="https://oss.imzhanghao.com/img/202211081512646.png" srcset="/img/loading.gif" lazyload alt="语义标签" /></p>
<h3 id="52-流程"><a class="markdownIt-Anchor" href="#52-流程"></a> 5.2 流程</h3>
<ul>
<li><strong>Contrastive pre-training</strong>：预训练阶段，使用图片 - 文本对进行对比学习训练；</li>
<li><strong>Create dataset classifier from label text</strong>：提取预测类别文本特征;</li>
<li><strong>Use for zero-shot predictiion</strong>：进行 Zero-Shoot 推理预测;</li>
</ul>
<p><img src="https://oss.imzhanghao.com/img/202211081515556.png" srcset="/img/loading.gif" lazyload alt="Contrastive pre-training" /></p>
<p><img src="https://oss.imzhanghao.com/img/202211081516232.png" srcset="/img/loading.gif" lazyload alt="Zero-shot" /></p>
<p><strong>阶段1：Contrastive pre-training</strong><br />
在预训练阶段，对比学习十分灵活，只需要定义好 正样本对 和 负样本对 就行了，其中能够配对的 image-text 对即为正样本。具体来说，先分别对图像和文本提特征，这时图像对应生成 I1、I2 … In 的特征向量（Image Feature），文本对应生成 T1、T2 … Tn 的特征向量（Text Feature），中间对角线为正样本，其余均为负样本。</p>
<p><strong>阶段2：Create dataset classifier from label text</strong><br />
基于400M数据上学得的先验，仅用数据集的标签文本，就可以得到很强的图像分类性能。现在训练好了，然后进入前向预测阶段，通过 prompt label text 来创建待分类的文本特征向量。</p>
<p><strong>阶段3：Use for zero-shot predictiion</strong><br />
最后就是推理见证效果的时候，对于测试图片，选择相似度最大的那个类别输出。在推理阶段，无论来了张什么样的图片，只要扔给 Image Encoder 进行特征提取，会生成一个一维的图片特征向量，然后拿这个图片特征和 N 个文本特征做余弦相似度对比，最相似的即为想要的那个结果，比如这里应该会得到 “A photo of a guacamole.”，</p>
<h3 id="53-实现"><a class="markdownIt-Anchor" href="#53-实现"></a> 5.3 实现</h3>
<p><img src="https://oss.imzhanghao.com/img/202211081543158.png" srcset="/img/loading.gif" lazyload alt="Numpy-like pseudocode for the core of an implementation of CLIP." /></p>
<h3 id="54-后续"><a class="markdownIt-Anchor" href="#54-后续"></a> 5.4 后续</h3>
<p><strong>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery (ICCV 2021 Oral)</strong><br />
StyleCLIP是结合CLIP和StyleGAN的一个工作，通过文字上的改变，从而去引导图像的生成。<br />
<a target="_blank" rel="noopener" href="https://github.com/orpatashnik/StyleCLIP">https://github.com/orpatashnik/StyleCLIP</a><br />
<img src="https://oss.imzhanghao.com/img/202211100602961.gif" srcset="/img/loading.gif" lazyload alt="StyleCLIP 例子" /></p>
<p><img src="https://oss.imzhanghao.com/img/202211100600578.png" srcset="/img/loading.gif" lazyload alt="StyleCLIP 例子" /></p>
<p><strong>CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders</strong><br />
CLIPDraw也是利用文字来指导图像生成的一个工作，只是想法更加简单，不需要进行模型训练，而是使用预训练的 CLIP 语言图像编码器作为度量，以最大化给定描述和生成的绘图之间的相似性，最后就可以生成很多简笔画的图像。<br />
<img src="https://oss.imzhanghao.com/img/202211100613302.png" srcset="/img/loading.gif" lazyload alt="CLIPDraw" /></p>
<p><strong>ViLD: Open-vocabulary Object Detection via Vision and Language Knowledge Distillation / Google</strong><br />
用CLIP来做物体检测和分割的任务，在CLIP出来一个月半月以后，就Google就出了这篇文章。<br />
作者指出，如果你用传统的物体检测方法，算法只能告诉你这些只是玩具，也就是下图蓝色的base categories，但是当你利用了这种自然语言之后，你就拜托了基础类的这个限制，就可以检测出来新的类，也就是红色的noval categories。<br />
<img src="https://oss.imzhanghao.com/img/202211100622320.png" srcset="/img/loading.gif" lazyload alt="ViLD" /></p>
<p><strong>CLIPasso: Semantically-Aware Object Sketching (SIGGRAPH 2022 Best Paper Award)</strong><br />
用CLIP提炼语义概念，生成图片目标的高度抽象线条画(速写)<br />
<img src="https://oss.imzhanghao.com/img/202211100641357.png" srcset="/img/loading.gif" lazyload alt="CLIPasso" /></p>
<p><strong>应用：Contrastive Language-Image Forensic Search</strong><br />
<a target="_blank" rel="noopener" href="https://github.com/johanmodin/clifs">https://github.com/johanmodin/clifs</a><br />
使用CLIP完成视频检索，看一个视频里面有没有出现过一个人或者一些场景，通过直接输入文本的这种形式进行检索。</p>
<p>A truck with the text “odwalla”<br />
<img src="https://oss.imzhanghao.com/img/202211100631474.png" srcset="/img/loading.gif" lazyload alt="A truck with the text &quot;odwalla&quot;" /></p>
<p>A white BMW car<br />
<img src="https://oss.imzhanghao.com/img/202211100631176.png" srcset="/img/loading.gif" lazyload alt="A white BMW car" /></p>
<h2 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h2>
<p>[1]<a target="_blank" rel="noopener" href="http://cje.ustb.edu.cn/cn/article/doi/10.13374/j.issn2095-9389.2019.03.21.003">多模态学习方法综述 / 陈鹏 / 工程科学学报 / 2019</a><br />
[2]<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning: A Survey and Taxonomy / Tadas Baltrusaitis / 2017</a><br />
[3]<a target="_blank" rel="noopener" href="https://cmu-multicomp-lab.github.io/mmml-course/fall2020/">MultiModal Machine Learning / Louis-Philippe Morency / CMU</a><br />
[4]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/543537800">CMU-10707 第二十一讲 多模态机器学习 / 华年ss / 知乎</a><br />
[5]<a target="_blank" rel="noopener" href="https://jmlr.org/papers/volume15/srivastava14b/srivastava14b.pdf">Multimodal Learning with Deep Boltzmann Machines / Nitish Srivastava / 2012</a><br />
[6]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/389287751?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=27272153137152&amp;utm_psn=1572265674642264064&amp;utm_source=wechat_session">多模态学习综述及最新方向 / yougeii​ / 知乎</a><br />
[7]<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.00020.pdf">Learning transferable visual models from natural language supervision / 2021</a><br />
[8]<a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP: Connecting Text and Images / openai / blog</a><br />
[9]<a target="_blank" rel="noopener" href="https://github.com/yzhuoning/Awesome-CLIP">Awesome-CLIP / yzhuoning</a></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/machinelearning/">机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0/">多模态学习</a>
                    
                      <a class="hover-with-bg" href="/tags/MultiModal-Learning/">MultiModal Learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              

              <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5495814268572722"
                  crossorigin="anonymous"></script>
              <ins class="adsbygoogle"
                  style="display:block"
                  data-ad-client="ca-pub-4128258433761966"
                  data-ad-slot="1528926940"
                  data-ad-format="auto"
                  data-full-width-responsive="true"></ins>
              <script>
                  (adsbygoogle = window.adsbygoogle || []).push({});
              </script>

              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/08/20/cognitions/">
                        <span class="hidden-mobile">关于认知的理解</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
    
  </div>
  

  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        陕ICP备20008129号
      </a>
    </span>
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css" />
  








  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?0c64a5657309290fd8f5efe33fcbcdb4";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'UA-166608124-1', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  
    <!-- Google gtag.js -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PKN25ZF7R2"></script>
    <script defer>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-PKN25ZF7R2');
    </script>
  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
